"""Sub-Query Executor Agent - Executes sub-queries using RAG engine."""

import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from src.rag.query_engine import RAGQueryEngine

from src.agents.models import AgentState
from src.rag.models import QueryResult

logger = logging.getLogger(__name__)


def sub_query_executor(state: AgentState, query_engine: "RAGQueryEngine") -> AgentState:
    """Execute sub-queries using the existing RAG query engine.

    This agent takes the sub-queries generated by the decomposer and executes
    each one using the RAG system. Execution can be parallel or sequential
    based on the decomposer's recommendation.

    Args:
        state: Current agent state containing sub_queries and execution_order
        query_engine: RAG query engine instance for executing queries

    Returns:
        Updated state with sub_results populated

    The executor:
    1. Retrieves sub-queries from state
    2. Executes queries in parallel or sequential mode
    3. Collects all QueryResult objects
    4. Records execution metadata
    5. Handles errors gracefully
    """
    start_time = time.time()

    sub_queries = state.get("sub_queries", [])
    execution_order = state.get("execution_order", "parallel")

    logger.info(
        f"Executor processing {len(sub_queries)} sub-queries ({execution_order} mode)"
    )

    # Initialize metadata fields if not present
    if "agent_calls" not in state:
        state["agent_calls"] = []
    if "reasoning_steps" not in state:
        state["reasoning_steps"] = []

    try:
        sub_results: list[QueryResult] = []

        if execution_order == "parallel":
            # Execute all sub-queries in parallel using ThreadPoolExecutor
            logger.debug("Executing sub-queries in parallel")

            # Use max_workers=3 to balance concurrency with API rate limits
            with ThreadPoolExecutor(max_workers=3) as executor:
                # Submit all queries
                future_to_query = {
                    executor.submit(query_engine.query, sub_q): sub_q
                    for sub_q in sub_queries
                }

                # Collect results as they complete
                for future in as_completed(future_to_query):
                    sub_q = future_to_query[future]
                    try:
                        result = future.result()
                        sub_results.append(result)
                        logger.debug(f"Completed sub-query: {sub_q[:50]}...")
                    except Exception as e:
                        logger.error(f"Sub-query execution failed: {sub_q}", exc_info=True)
                        # Create error result to maintain result ordering
                        error_result = QueryResult(
                            success=False,
                            answer=f"Error: {str(e)}",
                            sources=[],
                            chunks_retrieved=0,
                            query_time_seconds=0.0,
                            error_message=str(e),
                        )
                        sub_results.append(error_result)

        else:
            # Execute sub-queries sequentially
            logger.debug("Executing sub-queries sequentially")

            for i, sub_q in enumerate(sub_queries, 1):
                try:
                    logger.debug(f"Executing sub-query {i}/{len(sub_queries)}: {sub_q[:50]}...")
                    result = query_engine.query(sub_q)
                    sub_results.append(result)
                    logger.debug(
                        f"Sub-query {i} completed in {result.query_time_seconds:.2f}s"
                    )
                except Exception as e:
                    logger.error(f"Sub-query {i} execution failed: {sub_q}", exc_info=True)
                    # Create error result
                    error_result = QueryResult(
                        success=False,
                        answer=f"Error: {str(e)}",
                        sources=[],
                        chunks_retrieved=0,
                        query_time_seconds=0.0,
                        error_message=str(e),
                    )
                    sub_results.append(error_result)

        # Update state with results
        state["sub_results"] = sub_results

        # Record reasoning step
        duration_ms = int((time.time() - start_time) * 1000)

        # Calculate total chunks retrieved
        total_chunks = sum(result.chunks_retrieved for result in sub_results)

        state["reasoning_steps"].append(
            {
                "agent": "executor",
                "action": "sub_query_execution",
                "input": {
                    "sub_queries": sub_queries,
                    "execution_order": execution_order,
                },
                "output": {
                    "results_count": len(sub_results),
                    "total_chunks_retrieved": total_chunks,
                },
                "duration_ms": duration_ms,
            }
        )

        state["agent_calls"].append("executor")

        logger.info(
            f"Executor completed {len(sub_results)} sub-queries in {duration_ms}ms "
            f"({total_chunks} chunks retrieved)"
        )

    except Exception as e:
        # Handle unexpected errors
        logger.error(f"Executor unexpected error: {e}", exc_info=True)
        duration_ms = int((time.time() - start_time) * 1000)

        # Fallback: empty results
        state["sub_results"] = []

        state["reasoning_steps"].append(
            {
                "agent": "executor",
                "action": "execution_failed",
                "input": {"sub_queries": sub_queries},
                "output": {"error": str(e)},
                "duration_ms": duration_ms,
            }
        )

        state["agent_calls"].append("executor")
        state["error"] = f"Executor error: {str(e)}"

        logger.warning(f"Executor failed with error: {e}")

    return state
